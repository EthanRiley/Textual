{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc11884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "def code_mapping(df, src, targ):\n",
    "    \"\"\" map labels in src and targ columns to integers \"\"\"\n",
    "    labels = list(df[src]) + list(df[targ])\n",
    "    #labels = sorted(list(set(labels)))\n",
    "    \n",
    "    #print(labels)\n",
    "    \n",
    "    codes = list(range(len(labels)))\n",
    "    #print(codes)\n",
    "    \n",
    "    lcmap = dict(zip(labels, codes))\n",
    "    #print(lcmap)\n",
    "    \n",
    "    df = df.replace({src: lcmap, targ: lcmap})\n",
    "    #print(df)\n",
    "    \n",
    "    return df, labels\n",
    "\n",
    "\n",
    "def make_sankey(df, src, targ, vals=None, filename='1', **kwargs):\n",
    "    df, labels = code_mapping(df, src, targ)\n",
    "    \n",
    "    \n",
    "    if vals:\n",
    "        value = df[vals]\n",
    "    else:\n",
    "        value = [1] * df.shape[0]\n",
    "    \n",
    "    \n",
    "    link = {'source':df[src], 'target':df[targ], 'value':value}\n",
    "    \n",
    "    pad =kwargs.get('pad', 100)\n",
    "    thickness = kwargs.get('thickness', 10)\n",
    "    line_color = kwargs.get('line_color', 'black')\n",
    "    width = kwargs.get('width', 2)\n",
    "\n",
    "    node = {'pad':pad, 'thickness':thickness,\n",
    "            'line':{'color':line_color, 'width':width},\n",
    "            'label':labels}\n",
    "    \n",
    "    sk = go.Sankey(link=link, node=node)\n",
    "    fig = go.Figure(sk)\n",
    "    fig.write_image('sankey_{}.jpg'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834244bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from MakeSankey import make_sankey\n",
    "\n",
    "class textual:\n",
    "    \n",
    "    version = \"0.01\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = defaultdict(dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_polarity(speech):\n",
    "        '''\n",
    "        Takes in speech string and returns average polarity as float\n",
    "        Code by Rithesh\n",
    "        '''\n",
    "        sum1 = 0\n",
    "        # output list of sentences from speech \n",
    "        lst_of_sent = speech.split('.')\n",
    "        \n",
    "        # calculating avg sentence polarity \n",
    "        \n",
    "        for sent in lst_of_sent:\n",
    "            polar = TextBlob(sent).sentiment.polarity\n",
    "            sum1 += polar\n",
    "        return sum1 / len(lst_of_sent)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_subj(speech):\n",
    "        '''\n",
    "        Takes in speech string and returns average subjectivity as float\n",
    "        Code by Rithesh\n",
    "        '''\n",
    "        sum1 = 0\n",
    "        lst_of_sent = speech.split('.')\n",
    "        \n",
    "        # calculate avg sentence sebjectivity \n",
    "        for sent in lst_of_sent:\n",
    "            sub = TextBlob(sent).sentiment.subjectivity\n",
    "            sum1 += sub\n",
    "        return sum1 / len(lst_of_sent)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_punct(text):\n",
    "        '''\n",
    "        Filters punctuation and replaces with proper character (blank or space)\n",
    "        Code by Ethan and Rithesh\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        punctuation_to_blank = [',', ':', \";\", \"?\", '<', '>', \".\", \"'\", '\"', '`']\n",
    "        for character in punctuation_to_blank:\n",
    "            text = text.replace(character, '')\n",
    "\n",
    "        punctuation_to_space = ['\\n', '-']\n",
    "        for character in punctuation_to_space:\n",
    "            text = text.replace(character, ' ')\n",
    "            \n",
    "        text = text.split(' ')\n",
    "        new_list = []\n",
    "        for i in range(len(text)):\n",
    "            if text[i] != '':\n",
    "                new_list.append(text[i])\n",
    "            else:\n",
    "                pass\n",
    "        return new_list\n",
    "\n",
    "    @staticmethod\n",
    "    def find_avg_word_length(words):\n",
    "        '''\n",
    "        Finds average length of words\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        character_count = 0\n",
    "        word_count = 0\n",
    "        # counts all the characters and letters in the speech\n",
    "        for word in range(len(words)):\n",
    "            word_count += 1\n",
    "            character_count += len(words[word])\n",
    "        \n",
    "        return character_count / word_count\n",
    "\n",
    "    @staticmethod\n",
    "    def find_sentiment_count(words, sentiment_dict):\n",
    "        '''\n",
    "        Method from Ethan's president analysis assignment. Likely to be deleted\n",
    "        '''\n",
    "        count = 0\n",
    "        for word in words.keys():\n",
    "            if word in sentiment_dict:\n",
    "                count += words[word]\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_words(words, stop_words):\n",
    "        '''\n",
    "        Filters out stop words from dictionary of words\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        fwords = []\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                fwords.append(word)\n",
    "        return fwords\n",
    "\n",
    "    @staticmethod\n",
    "    def find_words_per_sentence(unfiltered, filtered):\n",
    "        '''\n",
    "        Find average sentence length\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        # Take unfiltered data and then count the splits on .\n",
    "        sentence_count = len(unfiltered.split('.'))\n",
    "        word_count = 0\n",
    "        # Count the words once more\n",
    "        for i in range(len(filtered)):\n",
    "            word_count += 1\n",
    "            \n",
    "        return word_count / sentence_count\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_per_100(words):\n",
    "        '''\n",
    "        Finds number of unique words per 100 words\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        unique_words = len(Counter(words).keys())\n",
    "        word_count = 0\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word_count += 1\n",
    "        \n",
    "        return (unique_words/word_count)*100\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_parser(filename):\n",
    "        '''\n",
    "        results = {\n",
    "            'wordcount': Counter('to be or not to be'.split(' ')),\n",
    "            'numwords': rnd.randrange(10,50)\n",
    "        }\n",
    "        Silly parser from class\n",
    "        '''\n",
    "        results = {}\n",
    "        return results\n",
    "    \n",
    "    def load_stop_words(self, stopfile):\n",
    "        '''\n",
    "        Loads stop words file for filtering method\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        f = open(stopfile, encoding='utf8')\n",
    "        raw = json.load(f)\n",
    "        self.stop_words = raw\n",
    "\n",
    "    def load_text(self, filename, label=None, parser=None):\n",
    "        if parser is None:\n",
    "            results = textual._default_parser(filename)\n",
    "        else:\n",
    "            results = parser(filename)\n",
    "\n",
    "        if label is None:\n",
    "            label = filename\n",
    "\n",
    "        for k, v in results.items():\n",
    "            self.data[k][label] = v \n",
    "\n",
    "    def compare_num_words(self):\n",
    "        num_words = self.data['numwords']\n",
    "        for label, nw in num_words.items():\n",
    "            plt.bar(label, nw)\n",
    "        plt.show\n",
    "\n",
    "    def make_sankey():\n",
    "        '''\n",
    "        Generates Sankey Diagram to show how words link together\n",
    "        Code by Ethan\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626175f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gilwo\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f924f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_words = [\"i\", \"a\", \"an\", \"of\", \"and\", \"to\", \"the\", \"is\", \"in\", \"it\", 'its', \"do\", 'for', 'while',\n",
    "             \"you\", \"your\", \"that\", \"me\", \"my\", \"i'd\", \"i'm\", \"by\", \"as\", 'from', 'than', 'on', 'when', 'upon'\n",
    "             \"but\", 'are', 'be', 'so', 'we', 'or', 'been', 'our', 'us', 'she', 'he', 'it', 'they', 'now',\n",
    "                'who', 'because', 'they', 'this', 'these', 'those', 'from', 'with', \"we'll\", 'what', 'than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a863c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
